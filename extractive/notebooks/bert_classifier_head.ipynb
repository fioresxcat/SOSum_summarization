{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manhdd5/anaconda3/envs/research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files for train, validation, and test datasets\n",
    "df = pd.read_csv('/media/manhdd5/4T/Manhdd/SOSum_summarization/data/train_dataset.csv')\n",
    "df['answer_body'] = df['answer_body'].str.replace('[©¥¢]', '', regex=True)\n",
    "df['truth'] = df['truth'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x != '' else [])\n",
    "\n",
    "val_df = pd.read_csv('/media/manhdd5/4T/Manhdd/SOSum_summarization/data/validation_dataset.csv')\n",
    "val_df['answer_body'] = val_df['answer_body'].str.replace('[©¥¢]', '', regex=True)\n",
    "val_df['truth'] = val_df['truth'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x != '' else [])\n",
    "\n",
    "test_df = pd.read_csv('/media/manhdd5/4T/Manhdd/SOSum_summarization/data/test_dataset.csv')\n",
    "test_df['answer_body'] = test_df['answer_body'].str.replace('[©¥¢]', '', regex=True)\n",
    "test_df['truth'] = test_df['truth'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x != '' else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode sentences using BERT\n",
    "def encode_sentence(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.pooler_output.squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step: Encode all sentences into embeddings\n",
    "def preprocess_data(dataframe):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        answer_sentences = row['answer_body'].split('.')\n",
    "        truth_indices = row['truth']\n",
    "        for i, sentence in enumerate(answer_sentences):\n",
    "            if sentence.strip():  # Skip empty sentences\n",
    "                sentences.append(sentence.strip())\n",
    "                labels.append(1 if i in truth_indices else 0)  # Label sentences\n",
    "                embeddings.append(encode_sentence(sentence.strip()))\n",
    "                \n",
    "    return np.array(embeddings), np.array(labels), sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the data\n",
    "# X, y = preprocess_data(df)\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data for train, validation, and test datasets\n",
    "X_train, y_train, train_sentences = preprocess_data(df)\n",
    "X_val, y_val, val_sentences = preprocess_data(val_df)\n",
    "X_test, y_test, test_sentences = preprocess_data(test_df)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP Classifier\": MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Random Forest\n",
      "\n",
      "Validation Set Results\n",
      "F1 Score: 0.39436619718309857\n",
      "Precision: 0.6461538461538462\n",
      "Recall: 0.28378378378378377\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.84      1482\n",
      "           1       0.65      0.28      0.39       592\n",
      "\n",
      "    accuracy                           0.75      2074\n",
      "   macro avg       0.71      0.61      0.62      2074\n",
      "weighted avg       0.73      0.75      0.72      2074\n",
      "\n",
      "\n",
      "Test Set Results\n",
      "F1 Score: 0.3554006968641115\n",
      "Precision: 0.6681222707423581\n",
      "Recall: 0.24208860759493672\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.95      0.84      1568\n",
      "           1       0.67      0.24      0.36       632\n",
      "\n",
      "    accuracy                           0.75      2200\n",
      "   macro avg       0.71      0.60      0.60      2200\n",
      "weighted avg       0.73      0.75      0.70      2200\n",
      "\n",
      "--------------------------------------------------\n",
      "Classifier: Decision Tree\n",
      "\n",
      "Validation Set Results\n",
      "F1 Score: 0.3783783783783784\n",
      "Precision: 0.3783783783783784\n",
      "Recall: 0.3783783783783784\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.75      0.75      1482\n",
      "           1       0.38      0.38      0.38       592\n",
      "\n",
      "    accuracy                           0.65      2074\n",
      "   macro avg       0.57      0.57      0.57      2074\n",
      "weighted avg       0.65      0.65      0.65      2074\n",
      "\n",
      "\n",
      "Test Set Results\n",
      "F1 Score: 0.4138473642800944\n",
      "Precision: 0.41158059467918623\n",
      "Recall: 0.4161392405063291\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.76      0.76      1568\n",
      "           1       0.41      0.42      0.41       632\n",
      "\n",
      "    accuracy                           0.66      2200\n",
      "   macro avg       0.59      0.59      0.59      2200\n",
      "weighted avg       0.66      0.66      0.66      2200\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manhdd5/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/manhdd5/anaconda3/envs/research/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "\n",
      "Validation Set Results\n",
      "F1 Score: 0.440713536201469\n",
      "Precision: 0.5817174515235457\n",
      "Recall: 0.3547297297297297\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83      1482\n",
      "           1       0.58      0.35      0.44       592\n",
      "\n",
      "    accuracy                           0.74      2074\n",
      "   macro avg       0.68      0.63      0.64      2074\n",
      "weighted avg       0.72      0.74      0.72      2074\n",
      "\n",
      "\n",
      "Test Set Results\n",
      "F1 Score: 0.44011684518013633\n",
      "Precision: 0.5721518987341773\n",
      "Recall: 0.3575949367088608\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83      1568\n",
      "           1       0.57      0.36      0.44       632\n",
      "\n",
      "    accuracy                           0.74      2200\n",
      "   macro avg       0.67      0.62      0.63      2200\n",
      "weighted avg       0.72      0.74      0.72      2200\n",
      "\n",
      "--------------------------------------------------\n",
      "Classifier: AdaBoost\n",
      "\n",
      "Validation Set Results\n",
      "F1 Score: 0.38505096262740657\n",
      "Precision: 0.584192439862543\n",
      "Recall: 0.28716216216216217\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.83      1482\n",
      "           1       0.58      0.29      0.39       592\n",
      "\n",
      "    accuracy                           0.74      2074\n",
      "   macro avg       0.67      0.60      0.61      2074\n",
      "weighted avg       0.71      0.74      0.71      2074\n",
      "\n",
      "\n",
      "Test Set Results\n",
      "F1 Score: 0.37831858407079644\n",
      "Precision: 0.6286764705882353\n",
      "Recall: 0.27056962025316456\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84      1568\n",
      "           1       0.63      0.27      0.38       632\n",
      "\n",
      "    accuracy                           0.74      2200\n",
      "   macro avg       0.69      0.60      0.61      2200\n",
      "weighted avg       0.72      0.74      0.71      2200\n",
      "\n",
      "--------------------------------------------------\n",
      "Classifier: Naive Bayes\n",
      "\n",
      "Validation Set Results\n",
      "F1 Score: 0.4318360914105595\n",
      "Precision: 0.40472673559822747\n",
      "Recall: 0.46283783783783783\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75      1482\n",
      "           1       0.40      0.46      0.43       592\n",
      "\n",
      "    accuracy                           0.65      2074\n",
      "   macro avg       0.59      0.60      0.59      2074\n",
      "weighted avg       0.67      0.65      0.66      2074\n",
      "\n",
      "\n",
      "Test Set Results\n",
      "F1 Score: 0.4069069069069069\n",
      "Precision: 0.3871428571428571\n",
      "Recall: 0.4287974683544304\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.74      1568\n",
      "           1       0.39      0.43      0.41       632\n",
      "\n",
      "    accuracy                           0.64      2200\n",
      "   macro avg       0.57      0.58      0.57      2200\n",
      "weighted avg       0.65      0.64      0.65      2200\n",
      "\n",
      "--------------------------------------------------\n",
      "Classifier: MLP Classifier\n",
      "\n",
      "Validation Set Results\n",
      "F1 Score: 0.45081967213114754\n",
      "Precision: 0.43789808917197454\n",
      "Recall: 0.46452702702702703\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77      1482\n",
      "           1       0.44      0.46      0.45       592\n",
      "\n",
      "    accuracy                           0.68      2074\n",
      "   macro avg       0.61      0.61      0.61      2074\n",
      "weighted avg       0.68      0.68      0.68      2074\n",
      "\n",
      "\n",
      "Test Set Results\n",
      "F1 Score: 0.4528\n",
      "Precision: 0.45792880258899676\n",
      "Recall: 0.4477848101265823\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.79      0.78      1568\n",
      "           1       0.46      0.45      0.45       632\n",
      "\n",
      "    accuracy                           0.69      2200\n",
      "   macro avg       0.62      0.62      0.62      2200\n",
      "weighted avg       0.69      0.69      0.69      2200\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each classifier on train set\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_val = clf.predict(X_val)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Classifier: {name}\")\n",
    "    \n",
    "    print(\"\\nValidation Set Results\")\n",
    "    print(f\"F1 Score: {f1_score(y_val, y_pred_val)}\")\n",
    "    print(f\"Precision: {precision_score(y_val, y_pred_val)}\")\n",
    "    print(f\"Recall: {recall_score(y_val, y_pred_val)}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val, y_pred_val))\n",
    "    \n",
    "    print(\"\\nTest Set Results\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred_test)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred_test)}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred_test)}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge_scores(pred_summary, true_summary):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(true_summary, pred_summary)\n",
    "    return scores\n",
    "\n",
    "# Function to reconstruct the predicted summary based on sentence indices\n",
    "def get_predicted_summary(sentences, y_pred):\n",
    "    return ' '.join([sentence for sentence, pred in zip(sentences, y_pred) if pred == 1])\n",
    "\n",
    "# Function to get true summary based on ground truth\n",
    "def get_true_summary(sentences, truth_indices):\n",
    "    return ' '.join([sentences[i] for i in truth_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate and display precision, recall, F1, ROUGE scores for a dataset\n",
    "def evaluate_with_rouge(dataset, classifier, dataset_name, X, y, sentences):\n",
    "    print(f\"Evaluating {dataset_name} set with {classifier}\")\n",
    "    y_pred = classifier.predict(X)\n",
    "    \n",
    "    # Classification metrics\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    \n",
    "    print(f\"{dataset_name} Set Results\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    \n",
    "    # ROUGE metrics\n",
    "    pred_summary = get_predicted_summary(sentences, y_pred)\n",
    "    true_summary = get_true_summary(sentences, dataset['truth'][0])  # Assuming 1 row per dataset for simplicity\n",
    "    \n",
    "    rouge_scores = calculate_rouge_scores(pred_summary, true_summary)\n",
    "    \n",
    "    print(f\"ROUGE-1: {rouge_scores['rouge1'].fmeasure}\")\n",
    "    print(f\"ROUGE-2: {rouge_scores['rouge2'].fmeasure}\")\n",
    "    print(f\"ROUGE-L: {rouge_scores['rougeL'].fmeasure}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Train set with RandomForestClassifier()\n",
      "Train Set Results\n",
      "F1 Score: 0.9623042721824859\n",
      "Precision: 0.9979951884522855\n",
      "Recall: 0.9290780141843972\n",
      "ROUGE-1: 0.00013401532241852986\n",
      "ROUGE-2: 8.934753959212848e-05\n",
      "ROUGE-L: 0.00013401532241852986\n",
      "--------------------------------------------------\n",
      "Evaluating Validation set with RandomForestClassifier()\n",
      "Validation Set Results\n",
      "F1 Score: 0.39436619718309857\n",
      "Precision: 0.6461538461538462\n",
      "Recall: 0.28378378378378377\n",
      "ROUGE-1: 0.0033064682785699526\n",
      "ROUGE-2: 0.0028943560057887122\n",
      "ROUGE-L: 0.0033064682785699526\n",
      "--------------------------------------------------\n",
      "Evaluating Test set with RandomForestClassifier()\n",
      "Test Set Results\n",
      "F1 Score: 0.3554006968641115\n",
      "Precision: 0.6681222707423581\n",
      "Recall: 0.24208860759493672\n",
      "ROUGE-1: 0.025229357798165136\n",
      "ROUGE-2: 0.011473152822395595\n",
      "ROUGE-L: 0.02064220183486239\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate ROUGE scores for train, validation, and test sets\n",
    "evaluate_with_rouge(df, classifiers['Random Forest'], \"Train\", X_train, y_train, train_sentences)\n",
    "evaluate_with_rouge(val_df, classifiers['Random Forest'], \"Validation\", X_val, y_val, val_sentences)\n",
    "evaluate_with_rouge(test_df, classifiers['Random Forest'], \"Test\", X_test, y_test, test_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
